Options and perf characteristics change all the time.  Make sure you
read the ext3 documentation.  As of this writing here is what we like
to use when creating ext3 filesystems:

$ mke2fs
    -v              # verbose
    -j              # with journal, ie, ext3
    -L SOME_LABEL   # you can refer to labels in /etc/fstab
    -O dir_index    # with directory index
    -m 2            # only 2% reserved for root
    -i 10240        # inode for every 10K of data
    -J size=400     # create large journal (see mail below)
    -b 4096         # block size
    -R stride=16    # stride * block size (should be =) RAID stripe size

'-i' should be what we think avg message size is for atleast the store
(blob file) directory.  I think our operating numbers are 30K - the
10K used above is from a test run to create a lot of small files.

Block size being bigger will cause waste with ext3 - not sure if we
want to make block size 8K or not.  Bigger block size better IO
throughput vs. more lost space.

Default journal mode is ordered which is what we want.

Also note caution in mail below about size of an ext3 file system.
Once we have (mailbox size) x (# users) we want to support on a specific
CPU profile, we should see if that number is too big and split the
'store' directory across multiple partitions.  We do not want to file
system check a 1TB file system and trust me our customers WILL end up
fsck'ing the thing - kernel bug, hardware problem etc.  If someone
measures this, please replace update this document.

If we are making filesystems smaller then we have to make sure the
2% is enough reserved space.

----------------------------------------------------------------------

https://listman.redhat.com/archives/ext3-users/2004-June/msg00016.html

From: "Phil White" <ext3 philwhite org>
To: "ext3" <ext3-users redhat com>
Subject: Re: mode data=journal in ext3. Is it safe to use?
Date: Wed, 16 Jun 2004 15:58:11 -0700 (PDT)

Petter,

I was never able to resolve the problems I had with data=journal with the
2.4 kernel.  I did *not* try the 2.6 kernel though, so I can't give you
any data points there.  In the end, I settled for data=ordered, and have
never seen the problems I described in my original posts.  Also, to give
you some background, I had been using ReiserFS before switching to ext3,
and I experienced a lot of corruption with Reiser (my company makes linux
based appliances which sometimes get turned off while under heavy IO).
Since ReiserFS doesn't do data journalling (metadata only), we
consistently ended up with corrupt files.  After this, I decided to try
ext3 with data=journal, and I never even got far enough with load testing
to try the 'hard reset' test.  It would consistently crash in the fs code
under heavy load.

We have since had no problems with data=ordered, and since it writes data
blocks before writing metadata to the journal, we don't see corrupt files
anymore (even on hard resets).

If data integrity (within the file) is important to you in the face of a
crash or power loss, do NOT use ReiserFS or ext3 data=writeback.   If your
application never overwrites data in files, you will be just fine using
data=ordered (appending to files or creating new files is pretty much
guaranteed to never cause corruption).  If you need to overwrite data in
files, you need to use data=journal (and probably beg people to fix it) or
rewrite your application to use some other method (i.e. copy the file,
delete the old one) and just use data=ordered.

--Phil

----------------------------------------------------------------------

From: Anand Palaniswamy
To: Satish Dharmaraj
Cc: Roland Schemers
Date: 7/30/2004
Subject: Re: Reiser vs. ext3

> reiser sucks ass. we use ext3. Anand can tell you all the stories
> about why....

One word answer: stability.

Fast but occassionally fucks up your filesystem is no good.  People do
claim "ReiserFS 3.0 was buggy when it came out, but now it's super
fast and stable."  I don't buy it, based on what I'm reading in the
ext3 and kernel mailing lists and our own Kluge said they had all
kinds of corruption with ReiserFS at Corvigo.

Redhat supports ext3 and I agree with them on this one:
    http://www.redhat.com/support/wpapers/redhat/ext3/why.html

From the RHEL3 mailing list answered by the guy that maintains Redhat
kernels:
    > Or is ext3 the officially supportd and recommended filesystem for all
    > RHEL3 installations?

    basically yes.

Also the point Redhat make, that e2fsck for file system checking is
better than everything else at recovering files, is valid.  We never
want to fsck a 1TB+ filesystem, but hey when disaster strikes...

Another anectode recently in LKML was when Linus decided to accept a
patch that made kernel stacks be 4K.  Reiser made such a fuss (they
had a large struct on stack), but finally sucked it up.  The number of
times Reiser gets flamed and turns around and gets all defensive on
the list is hilarious.

From the Oracle Linux FAQ:

        10. What File Systems are supported for Oracle on Linux?

        Supported filesystems are ext3 (with O_SYNC), NFS, OCFS and
        raw. ReiserFS is not supported.

The ReiserFS people are all now working on ReiserFS v4.  Reiser 4 will
be unusable for another 2 years.  They are already promoting Reiser4
aggressively not sure what their level of interest/support for Reiser3
is.

The one thing where ext3 really sucked was a directory with lot of
files, and with the dir_index changes (they maintain a btree of the
files in the directory) that issue has been addressed.  I'll take
careful programming with millions of seats over promising benchmark
throughput improvements.

At some point we can benchmark against ReiserFS and run some torture
tests on it.  Until we do that we can't confidenlty ask people to use
ReiserFS.

In summary, if this comes up, you should tell the customer:

        - If ReiserFS crashes who do you go to for support?  Do you
          want to buy a support contract from Reiser or from Redhat?
          Distributions haven't rallied behind ReiserFS and though
          some of this might be politics or egos, there seems to be a
          good measure of genuine concern about stability.

        - Our application will require "data=ordered" default ext3
          journaling mode.  (Write file data to disk, and then write
          the directory data (aka metadata) to journal, and then write
          the metadata to disk).  We will call syncs at the
          appropriate places to make sure messaging is robust.  The
          performance of "data=journal" which journals everything, and
          is slow, and other filesystems do this better, is not of
          concern to us.

        - We recommend and test with the dir_index "btree of directory
          data" ext3 option enabled.

        - We are concerned about the stability of anything other than
          ext3 from all the discussions we see and first hand hear
          about.  There are also "performance profiles changes
          drastically with differing workloads" concerns with XFS and
          JFS.  We want to recommend and support the most stable and
          widely deployed FS, but we will keep an open mind about
          testing and measuring the different options over time.

----------------------------------------------------------------------

From: Andreas Dilger <adilger@clusterfs.com>
Subject: Re: a few questions about ext3 journal
To: Jérôme Petazzoni <jp@enix.org>
Cc: ext3-users@redhat.com
Date: Fri, 11 Mar 2005 01:08:23 -0500

On Mar 10, 2005  12:10 +0100, Jérôme Petazzoni wrote:
> 1) Is there a way to check the size of the journal of an ext3 filesystem ?
> I mean - the actually used size ; not the total size of the journal.

There is no current statistics on any journal usage (though it would be
nice to have this).  Knowing how much space there currently is in the
journal, some sort of average of the free journal space (e.g. abs(head-tail)
as each new handle started), how often the journal was totally full and
had to be flushed, etc.  This would go a long way to telling a user and
the ext3 developers how large a journal is needed under their workload.

Currently Lustre just creates very large (400MB) journals on all of the
filesystems because we know that a large journal improves the performance
dramatically, but we have never done the trial+error approach of finding
the "optimal" size.

> 2) Would it be difficult to implement "freeze" of ext3 filesystem - that
> is, blocking all I/O to the filesystem until it's "unfrozen" (XFS can do
> that), for two purposes :
> A/ allowing "freezing" in a clean state, to allow clean snapshotting
> B/ allowing "freezing" while moving a SCSI disk or a network-connected
> disk without umounting filesystem

This is already done, and is used by the LVM/device mapper subsystem
to do snapshots of the filesystem.  However, I'm not sure if there is
a user-space API to access this.

> 3) Is it possible to allow data to stay in the journal for a very long
> time ?
> Rationale : for laptops with a lot of memory and some solid-state
> memory, this would allow to shutdown the hard disk (if all read data is
> in the cache, and all written data goes to the log on the solid-state disk).

Yes, this can be done (I think) by tuning the journal flush time and
having a large enough journal to avoid filling it up.  However, I
don't think this would be practical because the only common way to do
this would be e.g. flash memory and the heavy usage of the journal
would quickly wear out such devices, and it would also be slow.

Cheers, Andreas

----------------------------------------------------------------------

From: Andreas Dilger <adilger@clusterfs.com>
Subject: Re: Maximum ext3 file system size ??
To: Guolin Cheng <guolin@alexa.com>
Cc: Ext3-users@redhat.com
Date: Tue, 7 Dec 2004 01:57:31 -0700

On Dec 06, 2004  14:54 -0800, Guolin Cheng wrote:
>  If the ext3 file system maximum size updated or it is still 4TB for
> 2.6.* kernel?  The site at
> http://batleth.sapienti-sat.org/projects/FAQs/ext3-faq.html says that it
> is 4TB yet, but I would like to know if it is possible to create and use
> stable & easy-to-fix (or at least as stable & easy-to-fix as ext3) file
> systems as big as 100TB for 32 bit Linux architecture?

I don't think it is practical to have such gigantic filesystems for ext3,
even if it would be possible.  Currently for ia64 and ppc64 and Alpha you
could use larger blocksize (up to 64kB) to give up to 2^31 * 64kB = 2^47
or 128TB filesystems without (I think) any changes.  We had reports of
one user trying to use a 4TB ext3 filesystem but there were problems when
they wrote more than 2TB (though it was unclear whether the problems were
from ext3, MD RAID, or the block/SCSI layer).

However, with such extremely large filesystems the e2fsck time would be
incredibly large I think (it grows with block count and inode count).

Not to be self-serving, but Lustre (which uses ext3 as the back-end
filesystem) has several customers running with 100TB+ filesystems and
will have a 900TB installation next year.  It can do this by aggregating
multiple independent ext3 filesystems together, and also scales the
number of fileservers so that you have better performance in addition
to just a very large single-server filesystem.  It isn't for everyone
(a GPL version is available, but it isn't trivial to set up/use yet) but
it is reliable enough to use on half of the world's largest Linux systems.

Cheers, Andreas
--
Andreas Dilger
http://sourceforge.net/projects/ext2resize/
http://members.shaw.ca/adilger/             http://members.shaw.ca/golinux/
